{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec + SVM + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). You'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import urllib.request\n",
    "arr = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"data/review_train.json\") as f:\n",
    "    train = [json.loads(l) for l in f.readlines()]\n",
    "    train_reviews = arr([t['review'] for t in train])\n",
    "    train_labels = arr([t['label'] for t in train])\n",
    "with open(\"data/review_test.json\") as f:\n",
    "    test = [json.loads(l) for l in f.readlines()]\n",
    "    test_reviews = arr([t['review'] for t in test])\n",
    "    test_labels = arr([t['label'] for t in test])\n",
    "    id_to_label = {t['id']: t['label'] for t in test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the pre-trained term embeddings\n",
    "link = \"https://s3.eu-west-2.amazonaws.com/josiah-public-assets/glove.6B.50d.txt\"\n",
    "with urllib.request.urlopen(link, \"rb\") as lines:\n",
    "    w2vmodel = {line.split()[0].decode(): arr(line.split()[1:]).astype(float)\n",
    "           for line in lines}\n",
    "    \n",
    "model_words, model_vectors = zip(*w2vmodel.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review and remove stop words\n",
    "stops = arr(stopwords.words('english'))\n",
    "def clean(string):\n",
    "    tokens = arr(nltk.word_tokenize(string.lower()))\n",
    "    return np.extract(np.isin(tokens, stops, invert=True), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector representation for each review in the training data and testing data.\n",
    "get_vector_mean = lambda word_vecs: np.mean(word_vecs, axis=0)\n",
    "get_review_vector = lambda rev: get_vector_mean(arr([w2vmodel[word] for word in clean(rev) if word in w2vmodel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = arr([get_review_vector(r) for r in train_reviews])\n",
    "test_vectors = arr([get_review_vector(r) for r in test_reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving in to the coolness of word2vec\n",
    "- synonyms\n",
    "- combining words\n",
    "- analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_points(point, points):\n",
    "    points = np.asarray(points)\n",
    "    dist_2 = np.sum((points - point)**2, axis=1)\n",
    "    return sorted([(i, dist) for i, dist in enumerate(dist_2)], key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beverages',\n",
       " 'seafood',\n",
       " 'beverage',\n",
       " 'diet',\n",
       " 'drinks',\n",
       " 'dairy',\n",
       " 'meats',\n",
       " 'snacks',\n",
       " 'products']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try also experimenting with government, thursday, business, soldiers\n",
    "distances = closest_points(w2vmodel['foods'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['korean',\n",
       " 'pyongyang',\n",
       " 'dprk',\n",
       " 'seoul',\n",
       " 'japan',\n",
       " 'china',\n",
       " 'iran',\n",
       " 'beijing',\n",
       " 'koreans']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = closest_points(w2vmodel['korea'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['korea',\n",
       " 'products',\n",
       " 'beef',\n",
       " 'china',\n",
       " 'foods',\n",
       " 'export',\n",
       " 'states',\n",
       " 'imports',\n",
       " 'rice',\n",
       " 'taiwan']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add vectors to get more complex queries!\n",
    "distances = closest_points(w2vmodel['korea'] + w2vmodel['foods'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_vector = w2vmodel['king'] - w2vmodel['queen']\n",
    "distances = closest_points(dist_vector + w2vmodel['she'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'russia'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_vector = w2vmodel['spain'] - w2vmodel['madrid']\n",
    "distances = closest_points(dist_vector + w2vmodel['moscow'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talking'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_vector = w2vmodel['walking'] - w2vmodel['walked']\n",
    "distances = closest_points(dist_vector + w2vmodel['talked'], model_vectors)\n",
    "nearby_words = [model_words[idx] for idx, d in distances]\n",
    "nearby_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectors are in a 50 dimensional vectors space. First we have to do some PCA to bring it down to 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick two words that are moderately far apart...\n",
    "# \"torpedo\" and \"food\" about 11,000 words \"away\" (out of 40,000)\n",
    "group1_query = 'torpedo'\n",
    "group2_query = 'food'\n",
    "\n",
    "distances = closest_points(w2vmodel[group1_query], model_vectors)\n",
    "group1_words = [model_words[idx] for idx, d in distances][:100]\n",
    "group1_vectors = [w2vmodel[w] for w in group1_words]\n",
    "\n",
    "distances = closest_points(w2vmodel[group2_query], model_vectors)\n",
    "group2_words = [model_words[idx] for idx, d in distances][:100]\n",
    "group2_vectors = [w2vmodel[w] for w in group2_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a mapping to a low dimension representation of data\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit([*group1_vectors, *group2_vectors])\n",
    "x1, y1, z1 = pca.transform(group1_vectors).T\n",
    "x2, y2, z2 = pca.transform(group2_vectors).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of two words groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.1.1/libexec/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~josiahcoad/103.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter3d(\n",
    "    name=f'Words like {group1_query.upper()}',\n",
    "    x=x1,\n",
    "    y=y1,\n",
    "    z=z1,\n",
    "    text=group1_words,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker={'opacity': 0.8}\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter3d(\n",
    "    name=f'Words like {group2_query.upper()}',\n",
    "    x=x2,\n",
    "    y=y2,\n",
    "    z=z2,\n",
    "    text=group2_words,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker={'opacity': 0.8}\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    showlegend=True,\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='3d-scatter-colorscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize reviews\n",
    "Lets compare documents that are food-related versus not food-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 1000 # just take the first 1000 reviews\n",
    "short_reviews = [entry['review'][:100] + '...' for entry in train[:cut]]\n",
    "pca = PCA(n_components=3)\n",
    "x, y, z = pca.fit_transform(train_vectors[:cut]).T\n",
    "review_vectors = [np.array(t) for t in zip(x, y, z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.1.1/libexec/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~josiahcoad/103.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter3d(\n",
    "    x=x, y=y, z=z,\n",
    "    text=short_reviews,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        opacity=0.5,\n",
    "        color=train_labels[:cut]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    showlegend=True,\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='3d-scatter-colorscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      5975\n",
      "           1       0.90      0.93      0.92      5945\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     11920\n",
      "   macro avg       0.91      0.91      0.91     11920\n",
      "weighted avg       0.91      0.91      0.91     11920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(gamma=\"scale\", kernel=\"linear\")\n",
    "predictions = clf.fit(train_vectors, train_labels).predict(test_vectors)\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the SVM decision plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
